{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from shared import *            #shared functions from shared.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemic Similarity\n",
    "From here: [https://stackoverflow.com/questions/26474847/estimate-phonemic-similarity-between-two-words] \n",
    "\n",
    "Web browser view: [http://www.greenteapress.com/thinkpython/code/c06d?fbclid=IwAR3kK8u0l48ksaGi8v60FZLDsSjpdjhw3dCCeZdRDS0VkBhgeR5YyzSUTuI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cmu_sound_dict():\n",
    "    \"\"\"create a phonemic dictionary\"\"\"\n",
    "    \n",
    "    cmu_final_sound_dict = {}\n",
    "    with open('../c06d') as cmu_dict:\n",
    "        cmu_dict = cmu_dict.read().split(\"\\n\")\n",
    "        for i in cmu_dict:\n",
    "            i_s = i.split()\n",
    "            if len(i_s) > 1:\n",
    "                word = i_s[0]\n",
    "                syllables = i_s[1:]\n",
    "            cmu_final_sound_dict[word.lower()] = \" \".join(syllables)\n",
    "    return cmu_final_sound_dict\n",
    "\n",
    "phonemic_model = create_cmu_sound_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript Dictionary\n",
    "Create a dictionary for all words in transcript that contains a corresponding word with smallest Levenstein distance as well as distance itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tr_unigrm():\n",
    "    \"\"\"read in words from all the transcripts into a unique unigram list\"\"\"\n",
    "    \n",
    "    tr_unigrm_unique = []\n",
    "    with open('../tr_unigrams.txt', encoding='utf-8-sig') as tr_unigrm:\n",
    "        tr_unigrm = tr_unigrm.read().replace('\"','').replace('\\\\\\\\n', '').replace(\"\\\\\\\\t\", \"\")\n",
    "        tr_unigrm = tr_unigrm.split(\", \")\n",
    "        for i in range(len(tr_unigrm)):\n",
    "            if tr_unigrm[i] not in tr_unigrm_unique:\n",
    "                tr_unigrm_unique.append(tr_unigrm[i])\n",
    "    return tr_unigrm_unique\n",
    "\n",
    "tr_unigrm = create_tr_unigrm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tr_unigram_dict():\n",
    "    \"\"\"create a dictionary for all the unique unigrams from the tr_unigram list. \n",
    "    Dictionary will contain words as keys and values will have a list of the following elements: \n",
    "    a corresponding word that has a minimum Levenstein distance as well as the distance itself\"\"\"\n",
    "    \n",
    "    tr_unigrm_dict = {}\n",
    "    for j in range(len(tr_unigrm)):\n",
    "        if tr_unigrm[j] not in phonemic_model:  #if  unigram from transcript dictionary not in phonemic model return a random value from the reduced unigram list in shared file\n",
    "            tr_unigrm_dict[tr_unigrm[j]] = [random.choice(reduced_one_gram)[1], -1]   #set distance to -1      \n",
    "        else:                \n",
    "            temp_sub = []\n",
    "            temp_dist = []\n",
    "            for i in range(len(reduced_one_gram)):\n",
    "                if tr_unigrm[j] != reduced_one_gram[i][1]:\n",
    "                    if reduced_one_gram[i][1] in phonemic_model: \n",
    "                        temp_dist.append(nltk.edit_distance(phonemic_model[tr_unigrm[j]], phonemic_model[reduced_one_gram[i][1]], transpositions = False))\n",
    "                        temp_sub.append(reduced_one_gram[i][1])\n",
    "            tr_unigrm_dict[tr_unigrm[j]] = [temp_sub[temp_dist.index(min(temp_dist))], min(temp_dist)]\n",
    "    return tr_unigrm_dict\n",
    "\n",
    "tr_unigrm_dict = create_tr_unigram_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subst_words(transcript, rate):\n",
    "    \"\"\"substitute words in transcript at a given rate.\n",
    "    Return the new transcript and list of substituted words\"\"\"\n",
    "    \n",
    "    transcript = json.loads(transcript)\n",
    "    words_to_sub = random_words_list(flatten(transcript), rate)\n",
    "    substituted_words =[]\n",
    "    try:\n",
    "        while 0 != (len(words_to_sub)):\n",
    "            for sublist in transcript:                 \n",
    "                for element in sublist['tokens']:\n",
    "                     if element['type'] not in('SUB','RND'):      #avoid manipulating words that were already altered i.e. substituted                   \n",
    "                        if words_to_sub[0] == element['value']: \n",
    "                            substituted_words.append(element)                            \n",
    "                            if words_to_sub[0] in tr_unigrm_dict.keys():                                \n",
    "                                element['value'] = tr_unigrm_dict[words_to_sub[0]][0]\n",
    "                                element['type'] = 'SUB'\n",
    "                            else:\n",
    "                                element['value'] = random.choice(one_gram_list)[1]  # substitute with a random word from unigram\n",
    "                                element['type'] = 'RND'\n",
    "                            words_to_sub.remove(words_to_sub[0])\n",
    "                            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for sublist in transcript: \n",
    "        for element in sublist['tokens']:\n",
    "            if element['type'] in('SUB','RND'):\n",
    "                element['type'] = 'word'\n",
    "                \n",
    "    return json.dumps(transcript), substituted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_id</th>\n",
       "      <th>diarisation_id</th>\n",
       "      <th>json_utterances_man</th>\n",
       "      <th>json_utterances_asr</th>\n",
       "      <th>transcript_manual</th>\n",
       "      <th>json_utterances_man_with_SUBSTITUTED_WORDS_20%</th>\n",
       "      <th>SUBSTITUTED_WORDS_20%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1084</td>\n",
       "      <td>1080</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"thank\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you xxx xxx boston legal at sunday night...</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"thank\"...</td>\n",
       "      <td>[{'type': 'word', 'value': 'my'}, {'type': 'wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>1045</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"yes\"},...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes i read moby dick while i was making the mo...</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"yes\"},...</td>\n",
       "      <td>[{'type': 'word', 'value': 'no'}, {'type': 'wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1103</td>\n",
       "      <td>1099</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"I'm\"},...</td>\n",
       "      <td>[{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...</td>\n",
       "      <td>i'm b smith and i suffer from alzheimer diseas...</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"ponies...</td>\n",
       "      <td>[{'type': 'word', 'value': 'so'}, {'type': 'wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>396</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"we're\"...</td>\n",
       "      <td>[{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...</td>\n",
       "      <td>we're going to get to an ophthalmologist real ...</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"docks\"...</td>\n",
       "      <td>[{'type': 'word', 'value': 'there'}, {'type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1015</td>\n",
       "      <td>1011</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"so\"}, ...</td>\n",
       "      <td>[{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{\"tokens\": [{\"type\": \"word\", \"value\": \"no\"}, ...</td>\n",
       "      <td>[{'type': 'word', 'value': 'wanting'}, {'type'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   report_id  diarisation_id  \\\n",
       "0       1084            1080   \n",
       "1       1049            1045   \n",
       "2       1103            1099   \n",
       "3        400             396   \n",
       "4       1015            1011   \n",
       "\n",
       "                                 json_utterances_man  \\\n",
       "0  [{\"tokens\": [{\"type\": \"word\", \"value\": \"thank\"...   \n",
       "1  [{\"tokens\": [{\"type\": \"word\", \"value\": \"yes\"},...   \n",
       "2  [{\"tokens\": [{\"type\": \"word\", \"value\": \"I'm\"},...   \n",
       "3  [{\"tokens\": [{\"type\": \"word\", \"value\": \"we're\"...   \n",
       "4  [{\"tokens\": [{\"type\": \"word\", \"value\": \"so\"}, ...   \n",
       "\n",
       "                                 json_utterances_asr  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  [{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...   \n",
       "3  [{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...   \n",
       "4  [{\"tokens\": [{\"confidence\": 1.0, \"type\": \"unfi...   \n",
       "\n",
       "                                   transcript_manual  \\\n",
       "0  thank you xxx xxx boston legal at sunday night...   \n",
       "1  yes i read moby dick while i was making the mo...   \n",
       "2  i'm b smith and i suffer from alzheimer diseas...   \n",
       "3  we're going to get to an ophthalmologist real ...   \n",
       "4                                                NaN   \n",
       "\n",
       "      json_utterances_man_with_SUBSTITUTED_WORDS_20%  \\\n",
       "0  [{\"tokens\": [{\"type\": \"word\", \"value\": \"thank\"...   \n",
       "1  [{\"tokens\": [{\"type\": \"word\", \"value\": \"yes\"},...   \n",
       "2  [{\"tokens\": [{\"type\": \"word\", \"value\": \"ponies...   \n",
       "3  [{\"tokens\": [{\"type\": \"word\", \"value\": \"docks\"...   \n",
       "4  [{\"tokens\": [{\"type\": \"word\", \"value\": \"no\"}, ...   \n",
       "\n",
       "                               SUBSTITUTED_WORDS_20%  \n",
       "0  [{'type': 'word', 'value': 'my'}, {'type': 'wo...  \n",
       "1  [{'type': 'word', 'value': 'no'}, {'type': 'wo...  \n",
       "2  [{'type': 'word', 'value': 'so'}, {'type': 'wo...  \n",
       "3  [{'type': 'word', 'value': 'there'}, {'type': ...  \n",
       "4  [{'type': 'word', 'value': 'wanting'}, {'type'...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../ASRforAD.csv')\n",
    "\n",
    "#substitute % of words from json manual transcript with words that have shortest Levenstein distance\n",
    "df = df.merge(df.json_utterances_man.apply(lambda s: pd.Series(subst_words(s, 0.2))), left_index=True, right_index=True)\n",
    "df.rename(columns = {0:'json_utterances_man_with_SUBSTITUTED_WORDS_20%', 1:'SUBSTITUTED_WORDS_20%'}, inplace =True )\n",
    "\n",
    "#output csv with altered manual transcript and list of words that were substituted as new columns\n",
    "df.to_csv('../SUBSTITUTION_ASRforAD.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
