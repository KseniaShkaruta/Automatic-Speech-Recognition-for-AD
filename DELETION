import pandas as pd
import json
import statistics


#read in csv file and store asr transcript in temp value
df = pd.read_csv('C:/Users/toshiba/Documents/test.csv')
print(df.head(3))
tr = json.loads(df.iloc[0,2])

# function to count number of tokens
def tokens_count(transcript):
    return len(transcript)

# function to count number of words in transcript
def word_count(transcript):
    cnt = 0
    for sublist in tr:
        for i in range(len(sublist['tokens'])):
            if sublist['tokens'][i]['type'] == 'word':
                cnt +=1
    return print(cnt)

#some stats about words per token
word_in_token = []

for sublist in tr:
    cnt = 0
    for i in range(len(sublist['tokens'])):        
        if sublist['tokens'][i]['type'] == 'word':            
            cnt +=1
    word_in_token.append(cnt)
print(len(word_in_token))
print(min(word_in_token))
print(max(word_in_token))
print(statistics.mode(word_in_token))

#calculate number of words deleted per token proportionate to the token size
del_rate = 0.2
b=0

for sublist in tr:
    cnt = 0
    for i in range(len(sublist['tokens'])):        
        if sublist['tokens'][i]['type'] == 'word':            
            cnt +=1
    print(cnt)
    a = round(cnt*del_rate)
    print(a)
    b+=a

print(b)

# function to store transcript into two new lists
def transcript (transcript):
    for sublist in tr: 
        a = len(sublist['tokens'])
        del_lst = [[] for _ in range(a)] 
        new_lst = [[] for _ in range(a)] 
        m=0
        n=0
        
        for i in range(len(sublist['tokens'])):

            if sublist['tokens'][i]['value']== 'xxx':
               #print(sublist['tokens'][i])

                del_lst[m] = sublist['tokens'][i]
                m+=1
                   #print(temp_lst)
                   #print("this is the delete")
            else:
                new_lst[n] = sublist['tokens'][i]  
                n+=1
               # print(new_lst)
        return del_lst, new_lst
    
word_count(tr)
tokens_count(tr)
transcript(tr)
